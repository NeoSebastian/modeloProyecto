{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a02233-f16a-460f-b578-ea4d08f9ced3",
   "metadata": {},
   "source": [
    "# Cargar Solo un Modelo\n",
    "\n",
    "Este cuaderno tiene el codigo para cargar solo un modelo de los que ya estan preentrenados. Se construye el grafo, se agregan los embeddings de texto y con la arquitectura GraphSAGE se cargar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d4419-9b5f-4b35-b66e-d41a7803deed",
   "metadata": {},
   "source": [
    "## Instalaci칩n De Librerias y Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0afd79-00b2-4c07-b67a-8747acdceed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALACI칍N DE LIBRERIAS NECESARIAS\n",
    "\n",
    "!pip install networkx transformers pysentimiento pandas numpy scikit-learn matplotlib nltk seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7be3d0-a451-420e-84d1-c47defc1bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci칩n de PyTorch 2.6.0 con soporte para CUDA 12.4, incluyendo torchvision y torchaudio\n",
    "\n",
    "!pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66167e9-d8f3-4134-a8cb-5bbf666f20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci칩n de PyG (PyTorch Geometric) y sus dependencias compatibles con PyTorch 2.6.0 y CUDA 12.4\n",
    "\n",
    "!pip install torch-scatter==2.1.2+pt26cu124 torch-sparse==0.6.18+pt26cu124 torch-geometric==2.5.3 -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5ae2f-8eab-46af-b1e7-8abfa66d3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci칩n de pyg-lib 0.4.0 compatible con PyTorch 2.6.0 y CUDA 12.4 desde el repositorio oficial de PyG\n",
    "\n",
    "!pip install pyg-lib==0.4.0+pt26cu124 -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e732288-ac1f-4df2-a9e2-dcfd420df353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAR BIBILOTECAS\n",
    "\n",
    "# files se utiliza para subir los archivos .csv solo a google colab.\n",
    "\"\"\"\n",
    "from google.colab import files\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158b818a-3961-4eb6-a037-be2db83f177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.6.0+cu124\n",
      "12.4\n",
      "2.5.3\n"
     ]
    }
   ],
   "source": [
    "# Verifica si hay una GPU disponible, si la hay retorna True, sino False\n",
    "print(torch.cuda.is_available())   # Deber칤a mostrar True\n",
    "print(torch.__version__)  # Deber칤a mostrar '2.6.0' si PyTorch se instal칩 correctamente\n",
    "print(torch.version.cuda)  # Deber칤a mostrar '12.4', indicando que se est치 usando CUDA 12.4 (cu124)\n",
    "import torch_geometric\n",
    "print(torch_geometric.__version__)  # Deber칤a imprimir la versi칩n de PyTorch Geometric sin errores si est치 instalado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de523b-ff36-4231-bd23-cce5b2400f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subir los archivos CSV (SOLO en Google Colab)\n",
    "uploaded = files.upload()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01bf0-1bef-43ce-92b8-c2cd6c8ebc8a",
   "metadata": {},
   "source": [
    "### Cargar los .CSV a DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f540cc0-e084-4fd8-9f41-5588fda135c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    PARA GOOGLE COLAB\n",
    "\"\"\"\n",
    "emprendimientos = pd.read_csv('emprendimientos.csv')\n",
    "publicaciones = pd.read_csv('publicaciones.csv')\n",
    "comentarios = pd.read_csv('comentarios.csv')\n",
    "seguidores = pd.read_csv('seguidores.csv')\n",
    "emprendimiento_tematica = pd.read_csv('emprendimiento_tematica.csv')\n",
    "municipios = pd.read_csv('municipios.csv')\n",
    "alcances = pd.read_csv('alcance.csv')\n",
    "tematicas = pd.read_csv('tematicas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589d90d-6819-4fe2-9ed5-5282e3980c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    PARA JUPYTER NOTEBOOK\n",
    "\"\"\"\n",
    "emprendimientos = pd.read_csv('.\\DATOS\\emprendimientos.csv')\n",
    "publicaciones = pd.read_csv('.\\DATOS\\publicaciones.csv')\n",
    "comentarios = pd.read_csv('.\\DATOS\\comentarios.csv')\n",
    "seguidores = pd.read_csv('.\\DATOS\\seguidores.csv')\n",
    "emprendimiento_tematica = pd.read_csv('.\\DATOS\\emprendimiento_tematica.csv')\n",
    "municipios = pd.read_csv('.\\DATOS\\municipios.csv')\n",
    "alcances = pd.read_csv('.\\\\DATOS\\\\alcance.csv')\n",
    "tematicas = pd.read_csv('.\\\\DATOS\\\\tematicas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337948a1-5172-4dd8-9145-583094d0bd96",
   "metadata": {},
   "source": [
    "## Construcci칩n Del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2b280-0164-4d8f-8cf3-8ab3cfef1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "# Agregar nodos (emprendimientos)\n",
    "for _, row in emprendimientos.iterrows():\n",
    "    # Obtener tem치ticas asociadas\n",
    "    temas = emprendimiento_tematica[emprendimiento_tematica['id_emprendimiento'] == row['id_emprendimiento']]['id_tematica'].tolist()\n",
    "    # Atributos del nodo\n",
    "    node_attrs = {\n",
    "        'nombre_emprendimiento': row['nombre_emprendimiento'],\n",
    "        'descripcion': row['descripcion'] if pd.notna(row['descripcion']) else '',\n",
    "        'redes_sociales': row['redes_sociales'] if pd.notna(row['redes_sociales']) else '',\n",
    "        'sitio_web': row['sitio_web'],\n",
    "        'id_municipio_origen': row['id_municipio_origen'],\n",
    "        'id_alcance': row['id_alcance'],\n",
    "        'tematicas': temas\n",
    "    }\n",
    "    G.add_node(row['id_emprendimiento'], **node_attrs)\n",
    "\n",
    "print(f\"Nodos creados: {G.number_of_nodes()}\") # Imprime el n칰mero de nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040643d-c007-4e0b-9e34-5ba9eaa629d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1: Crear aristas basadas en interacciones\n",
    "\n",
    "for _, pub in publicaciones.iterrows():\n",
    "    id_emprendimiento = pub['id_emprendimiento']\n",
    "    # Conectar emprendimientos que comparten tem치ticas en publicaciones (simplificaci칩n, ya que no tenemos datos de qui칠n coment칩)\n",
    "    related_emprendimientos = emprendimiento_tematica[\n",
    "        emprendimiento_tematica['id_tematica'].isin(\n",
    "            emprendimiento_tematica[emprendimiento_tematica['id_emprendimiento'] == id_emprendimiento]['id_tematica']\n",
    "        )\n",
    "    ]['id_emprendimiento'].unique()\n",
    "    # Excluir el emprendimiento actual de la lista de relacionados\n",
    "    related_emprendimientos = related_emprendimientos[related_emprendimientos != id_emprendimiento]\n",
    " \n",
    "    for rel_emp in related_emprendimientos:\n",
    "        if rel_emp != id_emprendimiento and rel_emp in G.nodes:\n",
    "            weight = pub['n_likes'] if pd.notna(pub['n_likes']) else 0\n",
    "            G.add_edge(id_emprendimiento, rel_emp, weight=weight + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed148d77-3e7d-4bc7-911c-8954dde45f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 2: Crear aristas basadas en similitudes\n",
    "\n",
    "for emp1 in G.nodes:\n",
    "    for emp2 in G.nodes:\n",
    "        if emp1 < emp2:  # Evitar duplicados\n",
    "            emp1_attrs = G.nodes[emp1]\n",
    "            emp2_attrs = G.nodes[emp2]\n",
    "            # Similitud por tem치tica\n",
    "            temas_comunes = len(set(emp1_attrs['tematicas']) & set(emp2_attrs['tematicas']))\n",
    "            # Similitud por municipio y alcance\n",
    "            same_municipio = 1 if emp1_attrs['id_municipio_origen'] == emp2_attrs['id_municipio_origen'] else 0\n",
    "            same_alcance = 1 if emp1_attrs['id_alcance'] == emp2_attrs['id_alcance'] else 0\n",
    "            # Ponderar arista\n",
    "            weight = temas_comunes + same_municipio + same_alcance\n",
    "            if weight > 0:\n",
    "                G.add_edge(emp1, emp2, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f140e-5091-47d2-bc63-dceee7bbf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 3: Agregar peso basado en seguidores\n",
    "\n",
    "for _, row in seguidores.iterrows():\n",
    "    id_emprendimiento = row['id_emprendimiento']\n",
    "    if id_emprendimiento in G.nodes:\n",
    "        G.nodes[id_emprendimiento]['seguidores'] = row['cantidad']\n",
    "        # Aumentar peso de aristas existentes seg칰n seguidores\n",
    "        for emp2 in G.neighbors(id_emprendimiento):\n",
    "            G[id_emprendimiento][emp2]['weight'] += row['cantidad'] / 1000  # Normalizar\n",
    "\n",
    "print(f\"Aristas creadas: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f00d01-9526-4061-97df-10b8cd23fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar atributos num칠ricos\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Calcular total de likes por emprendimiento\n",
    "likes_por_emprendimiento = publicaciones.groupby('id_emprendimiento')['n_likes'].sum().reset_index()\n",
    "likes_por_emprendimiento.columns = ['id_emprendimiento', 'total_likes']\n",
    "\n",
    "# Combinar likes y seguidores\n",
    "emprendimientos_features = emprendimientos.merge(likes_por_emprendimiento, on='id_emprendimiento', how='left').fillna({'total_likes': 0})\n",
    "emprendimientos_features = emprendimientos_features.merge(seguidores[['id_emprendimiento', 'cantidad']], on='id_emprendimiento', how='left').fillna({'cantidad': 0})\n",
    "\n",
    "# Normalizar total_likes y cantidad\n",
    "numeric_features = scaler.fit_transform(emprendimientos_features[['total_likes', 'cantidad']])\n",
    "emprendimientos_features[['total_likes_norm', 'cantidad_norm']] = numeric_features\n",
    "\n",
    "# Codificar atributos categ칩ricos\n",
    "onehot_encoder_municipio = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "municipio_encoded = onehot_encoder_municipio.fit_transform(emprendimientos[['id_municipio_origen']])\n",
    "municipio_encoded_df = pd.DataFrame(municipio_encoded, columns=onehot_encoder_municipio.get_feature_names_out(['id_municipio_origen']))\n",
    "\n",
    "onehot_encoder_alcance = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "alcance_encoded = onehot_encoder_alcance.fit_transform(emprendimientos[['id_alcance']])\n",
    "alcance_encoded_df = pd.DataFrame(alcance_encoded, columns=onehot_encoder_alcance.get_feature_names_out(['id_alcance']))\n",
    "\n",
    "# Codificar tem치ticas (matriz binaria)\n",
    "tematica_encoded = np.zeros((len(emprendimientos), len(tematicas)))\n",
    "for _, row in emprendimiento_tematica.iterrows():\n",
    "    idx_emp = emprendimientos.index[emprendimientos['id_emprendimiento'] == row['id_emprendimiento']].tolist()[0]\n",
    "    idx_tem = tematicas.index[tematicas['id_tematica'] == row['id_tematica']].tolist()[0]\n",
    "    tematica_encoded[idx_emp, idx_tem] = 1\n",
    "tematica_encoded_df = pd.DataFrame(tematica_encoded, columns=[f'tematica_{i}' for i in tematicas['id_tematica']])\n",
    "\n",
    "# Combinar caracter칤sticas num칠ricas y categ칩ricas\n",
    "features_df = pd.concat([emprendimientos_features[['id_emprendimiento', 'total_likes_norm', 'cantidad_norm']],\n",
    "                         municipio_encoded_df, alcance_encoded_df, tematica_encoded_df], axis=1)\n",
    "\n",
    "# Agregar caracter칤sticas al grafo\n",
    "for _, row in features_df.iterrows():\n",
    "    if row['id_emprendimiento'] in G.nodes:\n",
    "        G.nodes[row['id_emprendimiento']]['features'] = row.drop('id_emprendimiento').values\n",
    "\n",
    "print(\"Dimensiones de las caracter칤sticas:\\n\", features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15137d5-8a4b-4305-bac9-ee6f6007c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ---------------------------------\n",
    "# 1. CARGAR DATOS Y EMBEDDINGS\n",
    "# ---------------------------------\n",
    "empr = pd.read_csv('./DATOS/emprendimientos.csv')\n",
    "pubs = pd.read_csv('./DATOS/publicaciones.csv')\n",
    "coms = pd.read_csv('./DATOS/comentarios.csv')\n",
    "\n",
    "def load_emb(path):\n",
    "    arr = np.load(path, allow_pickle=True)\n",
    "    if arr.dtype == object:\n",
    "        # convierte lista de vectores a array 2D float32\n",
    "        arr = np.vstack([np.array(x, dtype=np.float32) for x in arr])\n",
    "    return arr\n",
    "\n",
    "desc_embs = load_emb('./Embeddings/512tk/descripcion_embeddings.npy')\n",
    "cont_embs = load_emb('./Embeddings/512tk/contenido_embeddings.npy')\n",
    "comm_embs = load_emb('./Embeddings/512tk/comentario_embeddings.npy')\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "span_stop = set(stopwords.words('spanish'))\n",
    "\n",
    "# ---------------------------------\n",
    "# 2. PREPARAR EMBEDDINGS TEXTUALES\n",
    "# ---------------------------------\n",
    "W_DESC, W_PUB, W_COM = 0.5, 0.3, 0.2\n",
    "node_ids = sorted(G.nodes())\n",
    "num_nodes = len(node_ids)\n",
    "emb_dim    = desc_embs.shape[1]\n",
    "\n",
    "if num_nodes == 0:\n",
    "    raise ValueError(\"El grafo G no tiene nodos.\")\n",
    "\n",
    "# preallocate matriz de texto\n",
    "raw_text = np.zeros((num_nodes, emb_dim), dtype=np.float32)\n",
    "\n",
    "for i, nid in enumerate(node_ids):\n",
    "    # descripci칩n\n",
    "    row = empr[empr.id_emprendimiento == nid]\n",
    "    if not row.empty:\n",
    "        idx = row.index[0]\n",
    "        de  = desc_embs[idx]\n",
    "    else:\n",
    "        de  = np.zeros(emb_dim, dtype=np.float32)\n",
    "\n",
    "    # publicaciones\n",
    "    p_rows = pubs[pubs.id_emprendimiento == nid]\n",
    "    if not p_rows.empty:\n",
    "        idxs = p_rows.index.values\n",
    "        pe   = np.nanmean(cont_embs[idxs], axis=0)\n",
    "    else:\n",
    "        pe   = np.zeros(emb_dim, dtype=np.float32)\n",
    "\n",
    "    # comentarios\n",
    "    pub_ids = p_rows.id_publicacion.values\n",
    "    c_rows  = coms[coms.id_publicacion.isin(pub_ids)]\n",
    "    if not c_rows.empty:\n",
    "        idxs_c = c_rows.index.values\n",
    "        ce     = np.nanmean(comm_embs[idxs_c], axis=0)\n",
    "    else:\n",
    "        ce     = np.zeros(emb_dim, dtype=np.float32)\n",
    "\n",
    "    raw_text[i] = W_DESC*de + W_PUB*pe + W_COM*ce\n",
    "\n",
    "# reducir dimensionalidad texto a 128\n",
    "svd = TruncatedSVD(n_components=128, random_state=42)\n",
    "text_feats = svd.fit_transform(raw_text)\n",
    "\n",
    "for i, nid in enumerate(node_ids):\n",
    "    G.nodes[nid]['text_features'] = text_feats[i]\n",
    "\n",
    "# ---------------------------------\n",
    "# 3. ESCALAR FEATURES NUM칄RICAS\n",
    "# ---------------------------------\n",
    "# recopilar\n",
    "num_list, has_num = [], []\n",
    "for nid in node_ids:\n",
    "    f = G.nodes[nid].get('features')\n",
    "    if f is not None:\n",
    "        num_list.append(f)\n",
    "        has_num.append(nid)\n",
    "\n",
    "if num_list:\n",
    "    num_mat = np.vstack(num_list)\n",
    "    scaler  = MinMaxScaler().fit(num_mat)\n",
    "    scaled  = scaler.transform(num_mat)\n",
    "    for j, nid in enumerate(has_num):\n",
    "        G.nodes[nid]['scaled_num'] = scaled[j]\n",
    "\n",
    "# ---------------------------------\n",
    "# 4. COMBINAR NUM칄RICAS + TEXTO\n",
    "# ---------------------------------\n",
    "A, B = 0.4, 0.6\n",
    "for nid in node_ids:\n",
    "    num = G.nodes[nid].get('scaled_num',\n",
    "         np.zeros_like(text_feats[0], dtype=np.float32))\n",
    "    txt = G.nodes[nid]['text_features']\n",
    "    G.nodes[nid]['combined_features'] = np.hstack([A*num, B*txt])\n",
    "\n",
    "# ---------------------------------\n",
    "# 5. CREAR Data DE PyG\n",
    "# ---------------------------------\n",
    "x_list = [G.nodes[n]['combined_features'] for n in node_ids]\n",
    "x = torch.tensor(np.vstack(x_list), dtype=torch.float)\n",
    "\n",
    "edges, weights = [], []\n",
    "for u, v, e in G.edges(data=True):\n",
    "    w = e.get('weight', 1.0)\n",
    "    edges += [[u, v], [v, u]]\n",
    "    weights += [w, w]\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_attr  = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,\n",
    "    num_nodes=num_nodes,\n",
    "    node_ids=torch.tensor(node_ids, dtype=torch.long)\n",
    ")\n",
    "\n",
    "# Verificaci칩n\n",
    "print(f\"Nodos: {data.num_nodes}\")\n",
    "print(f\"Dims por nodo: {data.num_node_features}\")\n",
    "print(f\"Aristas (bidireccional): {data.edge_index.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca78e4a-5ff6-42eb-b07d-02f40875a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Verificar nodos en el grafo\n",
    "print(\"Nodos en G:\", list(G.nodes)[:10])\n",
    "print(\"N칰mero de nodos:\", G.number_of_nodes())\n",
    "\n",
    "# Crear un mapeo de nodos a 칤ndices contiguos (0, 1, ..., N-1)\n",
    "nodes = list(G.nodes)\n",
    "mapping = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# Verificar que todos los nodos en las aristas est칠n en el mapeo\n",
    "edges_to_remove = [e for e in G.edges if e[0] not in mapping or e[1] not in mapping]\n",
    "if edges_to_remove:\n",
    "    print(\"Eliminando aristas con nodos inv치lidos:\", edges_to_remove)\n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "# Actualizar edge_index y edge_attr\n",
    "edge_index_list = [[mapping[e[0]], mapping[e[1]]] for e in G.edges]\n",
    "edge_weight_list = [G[e[0]][e[1]]['weight'] for e in G.edges]\n",
    "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t()\n",
    "edge_weight = torch.tensor(edge_weight_list, dtype=torch.float)\n",
    "\n",
    "print(\"Edge index creado con 칠xito:\", edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c0374-677c-4eb9-a600-04e793253caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener caracter칤sticas de los nodos\n",
    "node_features = torch.tensor([G.nodes[n]['combined_features'] for n in sorted(G.nodes)], dtype=torch.float)\n",
    "\n",
    "# Crear objeto Data\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
    "\n",
    "print(f\"Dimensiones de node_features: {data.x.shape}\")\n",
    "print(f\"Dimensiones de edge_index: {data.edge_index.shape}\")\n",
    "print(f\"Dimensiones de edge_weight: {data.edge_attr.shape}\")\n",
    "print(f\"N칰mero de nodos: {data.num_nodes}\")\n",
    "print(f\"N칰mero de aristas: {data.num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b1ad4-0ee3-4f8a-bedd-fce96d7bd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizar node_features\n",
    "scaler = StandardScaler()\n",
    "data.x = torch.tensor(scaler.fit_transform(data.x), dtype=torch.float)\n",
    "\n",
    "# Verificar varianza de caracter칤sticas\n",
    "print(\"Media y desv. est치ndar de node_features:\", data.x.mean(), data.x.std())\n",
    "\n",
    "# Dividir las aristas\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.15,  # 15% para validaci칩n\n",
    "    num_test=0.15,  # 15% para prueba\n",
    "    is_undirected=True,  # Grafo no dirigido\n",
    "    add_negative_train_samples=True,  # Generar aristas negativas\n",
    "    neg_sampling_ratio=0.5  # Igual n칰mero de aristas negativas que positivas\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    print(f\"Aristas de entrenamiento positivas: {train_data.edge_label_index.shape}\")\n",
    "    print(f\"Aristas de validaci칩n: {val_data.edge_label_index.shape}\")\n",
    "    print(f\"Aristas de prueba: {test_data.edge_label_index.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al dividir aristas: {e}\")\n",
    "    print(\"N칰mero de aristas:\", data.num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650a45e-24f3-4baf-8cd1-9f802a0aa22d",
   "metadata": {},
   "source": [
    "## Cargar Modelo Preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a65179-be60-4f60-bbaf-63abb9477398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Definir la clase GraphSAGE (misma que en el entrenamiento)\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(2 * out_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_channels, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x_src, x_dst):\n",
    "        h = torch.cat([x_src, x_dst], dim=1)\n",
    "        return torch.sigmoid(self.predictor(h)).view(-1)\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Definir hiperpar치metros\n",
    "in_channels = 120  # Confirmado por la salida del modelo (SAGEConv(120, 256))\n",
    "hidden_channels = 256\n",
    "out_channels = 128\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model = GraphSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
    "\n",
    "# Cargar el archivo .pth\n",
    "model_path = r\"D:\\Universidad\\Proyecto De Grado\\Construccion del modelo\\Modelo_Repositorio\\Modelos\\Segundo Metodo Modelos\\model_5_20250627_184503.pth\"\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Extraer el state_dict del diccionario cargado\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Poner el modelo en modo evaluaci칩n\n",
    "\n",
    "# Usar test_data del entrenamiento para las predicciones\n",
    "data = test_data.to(device)  # Mover test_data al dispositivo\n",
    "\n",
    "# Obtener embeddings y realizar predicciones\n",
    "with torch.no_grad():\n",
    "    h = model(data.x, data.edge_index)  # Generar embeddings para todos los nodos\n",
    "    pred = model.predict(h[data.edge_label_index[0]], h[data.edge_label_index[1]])  # Predecir para las aristas\n",
    "    pred_np = pred.cpu().numpy()  # Convertir a numpy\n",
    "    pred_binary = (pred_np > 0.5).astype(int)  # Predicciones binarias (umbral 0.5)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Predicciones (probabilidades):\", pred_np)\n",
    "print(\"Predicciones (binarias):\", pred_binary)\n",
    "\n",
    "# Opcional: Comparar con etiquetas reales (si est치n disponibles)\n",
    "if hasattr(data, 'edge_label'):\n",
    "    y_np = data.edge_label.cpu().numpy()\n",
    "    print(\"Etiquetas reales:\", y_np)\n",
    "    # Calcular m칠tricas\n",
    "    from sklearn.metrics import precision_score, f1_score, recall_score, roc_auc_score, average_precision_score\n",
    "    auc = roc_auc_score(y_np, pred_np)\n",
    "    ap = average_precision_score(y_np, pred_np)\n",
    "    precision = precision_score(y_np, pred_binary)\n",
    "    recall = recall_score(y_np, pred_binary)\n",
    "    f1 = f1_score(y_np, pred_binary)\n",
    "    print(f\"M칠tricas de prueba: Precision: {precision:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, Recall: {recall:.4f}, AP: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda3a4a-0af3-45f6-9e00-27736625046a",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dfd80-8b3e-4b2c-bdc4-c4dd58766c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_emprendimiento_info(id_emprendimiento, emprendimientos_df):\n",
    "    \"\"\"Obtiene informaci칩n de un emprendimiento dado su ID\"\"\"\n",
    "    emp = emprendimientos_df[emprendimientos_df['id_emprendimiento'] == id_emprendimiento].iloc[0]\n",
    "    return {\n",
    "        'id': id_emprendimiento,\n",
    "        'nombre': emp['nombre_emprendimiento'],\n",
    "        'descripcion': emp['descripcion'],\n",
    "        'municipio': emp['id_municipio_origen'],\n",
    "        'alcance': emp['id_alcance']\n",
    "    }\n",
    "\n",
    "def recommend_emprendimientos(model, data, emprendimientos_df, target_id, top_k=15):\n",
    "    \"\"\"\n",
    "    Recomienda emprendimientos similares a uno dado\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo GraphSAGE entrenado\n",
    "        data: Datos del grafo (en formato PyG)\n",
    "        emprendimientos_df: DataFrame con informaci칩n de emprendimientos\n",
    "        target_id: ID del emprendimiento objetivo\n",
    "        top_k: N칰mero de recomendaciones a devolver\n",
    "    \"\"\"\n",
    "    # Verificar dispositivo del modelo\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Verificar si el emprendimiento objetivo existe en el DataFrame\n",
    "    if target_id not in emprendimientos_df['id_emprendimiento'].values:\n",
    "        print(f\"Error: No se encontr칩 el emprendimiento con ID {target_id} en el DataFrame\")\n",
    "        return None\n",
    "    \n",
    "    # Mover datos al mismo dispositivo que el modelo\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Obtener embeddings de todos los nodos\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(data.x, data.edge_index).cpu().numpy()\n",
    "    \n",
    "    # Asumimos que el orden de los nodos en data.x corresponde a los IDs ordenados\n",
    "    # Si no es as칤, necesitar칤amos mapear los 칤ndices a los IDs\n",
    "    node_ids = emprendimientos_df['id_emprendimiento'].values\n",
    "    \n",
    "    try:\n",
    "        target_idx = np.where(node_ids == target_id)[0][0]\n",
    "    except IndexError:\n",
    "        print(f\"Error: El ID {target_id} no existe en los nodos del grafo\")\n",
    "        return None\n",
    "    \n",
    "    # Calcular similitud coseno\n",
    "    target_embedding = embeddings[target_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(target_embedding, embeddings)[0]\n",
    "    \n",
    "    # Ordenar por similitud (excluyendo el propio emprendimiento)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    sorted_indices = [i for i in sorted_indices if node_ids[i] != target_id][:top_k]\n",
    "    \n",
    "    # Obtener informaci칩n de los emprendimientos recomendados\n",
    "    recommendations = []\n",
    "    for idx in sorted_indices:\n",
    "        emp_id = node_ids[idx]\n",
    "        similarity = similarities[idx]\n",
    "        emp_info = get_emprendimiento_info(emp_id, emprendimientos_df)\n",
    "        emp_info['similitud'] = f\"{similarity:.3f}\"\n",
    "        recommendations.append(emp_info)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "    \n",
    "def print_recommendations(target_emp, recommendations):\n",
    "    \"\"\"Muestra las recomendaciones de forma legible\"\"\"\n",
    "    print(f\"\\nRecomendaciones para el emprendimiento:\")\n",
    "    print(f\"ID: {target_emp['id']}\")\n",
    "    print(f\"Nombre: {target_emp['nombre']}\")\n",
    "    print(f\"Descripci칩n: {target_emp['descripcion']}\")\n",
    "    print(f\"Municipio: {target_emp['municipio']}\")\n",
    "    print(f\"Alcance: {target_emp['alcance']}\")\n",
    "    \n",
    "    print(\"\\nEmprendimientos recomendados:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n#{i} (Similitud: {rec['similitud']})\")\n",
    "        print(f\"ID: {rec['id']}\")\n",
    "        print(f\"Nombre: {rec['nombre']}\")\n",
    "        print(f\"Descripci칩n: {rec['descripcion']}\")\n",
    "        print(f\"Municipio: {rec['municipio']}\")\n",
    "        print(f\"Alcance: {rec['alcance']}\")\n",
    "\n",
    "# Ejemplo de uso despu칠s del entrenamiento:\n",
    "# 1. Cargar el dataframe de emprendimientos (debes tenerlo cargado como emprendimientos_df)\n",
    "# emprendimientos_df = pd.read_csv('ruta/a/tu/archivo.csv')\n",
    "\n",
    "# 2. Seleccionar un emprendimiento para recomendar\n",
    "# target_id = 5  # Cambia esto por el ID que quieras probar\n",
    "\n",
    "# 3. Obtener recomendaciones\n",
    "# target_emp = get_emprendimiento_info(target_id, emprendimientos_df)\n",
    "# recommendations = recommend_emprendimientos(model, data, emprendimientos_df, target_id)\n",
    "\n",
    "# 4. Mostrar resultados\n",
    "# print_recommendations(target_emp, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520fd33d-460b-4992-827d-f5be0016b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Cargar el dataframe de emprendimientos\n",
    "emprendimientos_df = pd.read_csv('.\\DATOS\\emprendimientos.csv')\n",
    "\n",
    "# Generar recomendaciones\n",
    "target_id = 46 # Cambia esto por el ID que quieras probar\n",
    "target_emp = get_emprendimiento_info(target_id, emprendimientos_df)\n",
    "recommendations = recommend_emprendimientos(model, data, emprendimientos_df, target_id)\n",
    "\n",
    "if recommendations:\n",
    "    print_recommendations(target_emp, recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7dd42-a24c-44d7-9693-660b485a8314",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1d038-77fe-41a9-aef6-450333846008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import community as community_louvain  # Para Louvain\n",
    "from collections import Counter\n",
    "import plotly.express as px  # Para visualizaciones interactivas\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def detect_communities_with_embeddings(model, G, test_data, emprendimientos_df, tematicas_df, emprendimiento_tematica_df, n_clusters=5, resolution=1.0, use_3d=False):\n",
    "    \"\"\"\n",
    "    Detecta comunidades con m칰ltiples m칠todos de clustering y visualizaciones avanzadas.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo GraphSAGE entrenado\n",
    "        G: Grafo NetworkX\n",
    "        test_data: Objeto Data de PyTorch Geometric con x, edge_index, edge_label_index\n",
    "        emprendimientos_df: DataFrame con datos de emprendimientos\n",
    "        tematicas_df: DataFrame con datos de tem치ticas\n",
    "        emprendimiento_tematica_df: DataFrame con relaciones emprendimiento-tem치tica\n",
    "        n_clusters: N칰mero de clusters para K-Means y Agglomerative\n",
    "        resolution: Resoluci칩n para Louvain\n",
    "        use_3d: Si True, genera visualizaci칩n 3D interactiva\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame con asignaciones de clusters y comunidades\n",
    "        embeddings: Embeddings generados por el modelo\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 1. Generar embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(test_data.x.to(device), test_data.edge_index.to(device)).cpu().numpy()\n",
    "    \n",
    "    # 2. M칰ltiples m칠todos de clustering\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(z)\n",
    "    kmeans_silhouette = silhouette_score(z, kmeans_labels) if n_clusters > 1 else 0\n",
    "    kmeans_db = davies_bouldin_score(z, kmeans_labels) if n_clusters > 1 else 0\n",
    "\n",
    "    # Agglomerative Clustering\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    agg_labels = agglomerative.fit_predict(z)\n",
    "    agg_silhouette = silhouette_score(z, agg_labels) if n_clusters > 1 else 0\n",
    "    agg_db = davies_bouldin_score(z, agg_labels) if n_clusters > 1 else 0\n",
    "\n",
    "    # DBSCAN (para detectar outliers)\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(z)\n",
    "    n_dbscan_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    dbscan_silhouette = silhouette_score(z, dbscan_labels) if n_dbscan_clusters > 1 else 0\n",
    "\n",
    "    # 3. Detecci칩n de comunidades con Louvain\n",
    "    partition = community_louvain.best_partition(G.to_undirected(), weight='weight', resolution=resolution)\n",
    "    louvain_labels = np.array([partition[node] for node in G.nodes()])\n",
    "    n_louvain_communities = len(set(partition.values()))\n",
    "\n",
    "    # 4. Preparar datos para an치lisis\n",
    "    results_df = pd.DataFrame({\n",
    "        'id_emprendimiento': list(G.nodes()),\n",
    "        'cluster_kmeans': kmeans_labels,\n",
    "        'cluster_agglomerative': agg_labels,\n",
    "        'cluster_dbscan': dbscan_labels,\n",
    "        'comunidad_louvain': louvain_labels\n",
    "    }).merge(\n",
    "        emprendimientos_df[['id_emprendimiento', 'nombre_emprendimiento', 'descripcion', 'id_municipio_origen', 'id_alcance']],\n",
    "        on='id_emprendimiento'\n",
    "    ).merge(\n",
    "        seguidores[['id_emprendimiento', 'cantidad']],\n",
    "        on='id_emprendimiento',\n",
    "        how='left'\n",
    "    ).fillna({'cantidad': 0})\n",
    "\n",
    "    # 5. An치lisis de centralidad\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "    results_df['degree_centrality'] = [degree_centrality[node] for node in results_df['id_emprendimiento']]\n",
    "    results_df['betweenness_centrality'] = [betweenness_centrality[node] for node in results_df['id_emprendimiento']]\n",
    "\n",
    "    # 6. An치lisis de tem치ticas por comunidad\n",
    "    comunidad_tematica = []\n",
    "    for com_id in set(louvain_labels):\n",
    "        emps = results_df[results_df['comunidad_louvain'] == com_id]['id_emprendimiento']\n",
    "        temas = emprendimiento_tematica_df[emprendimiento_tematica_df['id_emprendimiento'].isin(emps)]['id_tematica']\n",
    "        tema_counts = temas.value_counts()\n",
    "        top_temas = tema_counts.head(5).index\n",
    "        tema_names = [tematicas_df[tematicas_df['id_tematica'] == t]['nombre'].values[0] for t in top_temas]\n",
    "        comunidad_tematica.append({\n",
    "            'comunidad': com_id,\n",
    "            'tematicas': ', '.join(tema_names),\n",
    "            'tematica_counts': tema_counts.to_dict()\n",
    "        })\n",
    "\n",
    "    # 7. Visualizaciones avanzadas\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # Gr치fico 1: Embeddings 2D con TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    z_2d = tsne.fit_transform(z)\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=kmeans_labels, cmap='viridis', s=results_df['cantidad'] / 10 + 50, alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Cluster K-Means')\n",
    "    central_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for node, _ in central_nodes:\n",
    "        idx = list(G.nodes()).index(node)\n",
    "        plt.text(z_2d[idx, 0], z_2d[idx, 1], \n",
    "                 results_df[results_df['id_emprendimiento'] == node]['nombre_emprendimiento'].values[0],\n",
    "                 fontsize=8, ha='center', va='bottom')\n",
    "    plt.title('Clusters K-Means en Espacio de Embeddings (TSNE)')\n",
    "    plt.xlabel('Dimensi칩n 1')\n",
    "    plt.ylabel('Dimensi칩n 2')\n",
    "\n",
    "    # Gr치fico 2: Red con comunidades Louvain\n",
    "    plt.subplot(2, 2, 2)\n",
    "    pos = nx.spring_layout(G, weight='weight', seed=42)\n",
    "    node_sizes = [results_df[results_df['id_emprendimiento'] == n]['cantidad'].values[0] / 10 + 50 for n in G.nodes()]\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, node_color=louvain_labels, node_size=node_sizes, cmap='tab20', alpha=0.8\n",
    "    )\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.1, edge_color='gray')\n",
    "    for node, (x, y) in pos.items():\n",
    "        if degree_centrality[node] > np.percentile(list(degree_centrality.values()), 75):\n",
    "            plt.text(x, y, \n",
    "                     results_df[results_df['id_emprendimiento'] == node]['nombre_emprendimiento'].values[0],\n",
    "                     fontsize=8, ha='center', va='center')\n",
    "    plt.title('Red de Emprendimientos (Comunidades Louvain)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Gr치fico 3: Mapa de calor de similitud entre comunidades\n",
    "    plt.subplot(2, 2, 3)\n",
    "    community_matrix = np.zeros((n_louvain_communities, n_louvain_communities))\n",
    "    for u, v in G.edges():\n",
    "        com_u, com_v = partition[u], partition[v]\n",
    "        community_matrix[com_u, com_v] += G[u][v].get('weight', 1.0)\n",
    "        community_matrix[com_v, com_u] += G[u][v].get('weight', 1.0)\n",
    "    sns.heatmap(community_matrix, cmap='Blues', annot=True, fmt='.0f')\n",
    "    plt.title('Matriz de Conexi칩n entre Comunidades')\n",
    "    plt.xlabel('Comunidad')\n",
    "    plt.ylabel('Comunidad')\n",
    "\n",
    "    # Gr치fico 4: Distribuci칩n de seguidores por comunidad\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='comunidad_louvain', y='cantidad', data=results_df)\n",
    "    plt.title('Distribuci칩n de Seguidores por Comunidad')\n",
    "    plt.xlabel('Comunidad Louvain')\n",
    "    plt.ylabel('N칰mero de Seguidores')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizaci칩n 3D interactiva (opcional)\n",
    "    if use_3d:\n",
    "        z_3d = PCA(n_components=3).fit_transform(z)\n",
    "        fig = px.scatter_3d(\n",
    "            x=z_3d[:, 0], y=z_3d[:, 1], z=z_3d[:, 2],\n",
    "            color=kmeans_labels, size=results_df['cantidad'] / 10 + 50,\n",
    "            text=results_df['nombre_emprendimiento'],\n",
    "            title='Clusters K-Means en 3D (PCA)'\n",
    "        )\n",
    "        fig.update_traces(textposition='top center')\n",
    "        fig.show()\n",
    "\n",
    "    # 8. An치lisis detallado\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"AN츼LISIS DETALLADO DE COMUNIDADES Y CLUSTERS\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # M칠tricas de clustering\n",
    "    print(\"\\n游늵 M칠tricas de Clustering:\")\n",
    "    print(f\"K-Means - Silhouette: {kmeans_silhouette:.4f}, Davies-Bouldin: {kmeans_db:.4f}\")\n",
    "    print(f\"Agglomerative - Silhouette: {agg_silhouette:.4f}, Davies-Bouldin: {agg_db:.4f}\")\n",
    "    print(f\"DBSCAN - N춿 Clusters: {n_dbscan_clusters}, Silhouette: {dbscan_silhouette:.4f}\")\n",
    "\n",
    "    # Solapamiento entre K-Means y Louvain\n",
    "    overlap = pd.crosstab(results_df['cluster_kmeans'], results_df['comunidad_louvain'])\n",
    "    print(\"\\n游댃 Solapamiento entre K-Means y Louvain:\")\n",
    "    print(overlap)\n",
    "\n",
    "    # An치lisis por comunidad Louvain\n",
    "    for com_id in sorted(set(louvain_labels)):\n",
    "        print(f\"\\n游댯 COMUNIDAD {com_id} ({len(results_df[results_df['comunidad_louvain'] == com_id])} miembros)\")\n",
    "        com_data = results_df[results_df['comunidad_louvain'] == com_id]\n",
    "        \n",
    "        # Tem치ticas\n",
    "        temas_info = comunidad_tematica[com_id]\n",
    "        print(f\"游늷 Tem치ticas principales: {temas_info['tematicas']}\")\n",
    "        print(f\"游늵 Distribuci칩n de tem치ticas: {temas_info['tematica_counts']}\")\n",
    "\n",
    "        # Estad칤sticas\n",
    "        print(f\"游늸 Municipio m치s com칰n: {com_data['id_municipio_origen'].mode()[0]}\")\n",
    "        print(f\"游깷 Alcance predominante: {'Local' if com_data['id_alcance'].mode()[0] == 1 else 'Regional'}\")\n",
    "        print(f\"游논 Seguidores totales: {com_data['cantidad'].sum():,}\")\n",
    "        print(f\"游늳 Centralidad promedio (degree): {com_data['degree_centrality'].mean():.4f}\")\n",
    "        print(f\"游늳 Centralidad promedio (betweenness): {com_data['betweenness_centrality'].mean():.4f}\")\n",
    "\n",
    "        # Emprendimientos destacados\n",
    "        top_emps = com_data.sort_values('cantidad', ascending=False).head(5)\n",
    "        print(\"\\n游 Emprendimientos destacados:\")\n",
    "        for _, emp in top_emps.iterrows():\n",
    "            print(f\"- {emp['nombre_emprendimiento']} ({emp['cantidad']:,} seguidores, Centralidad: {emp['degree_centrality']:.4f})\")\n",
    "\n",
    "        # An치lisis de clusters dentro de la comunidad\n",
    "        cluster_counts = com_data['cluster_kmeans'].value_counts()\n",
    "        print(\"\\n游늵 Distribuci칩n de clusters K-Means en esta comunidad:\")\n",
    "        for cluster_id, count in cluster_counts.items():\n",
    "            print(f\"Cluster {cluster_id}: {count} miembros\")\n",
    "\n",
    "    # An치lisis de cohesi칩n\n",
    "    print(\"\\n游댕 An치lisis de Cohesi칩n del Grafo:\")\n",
    "    n_components, _ = connected_components(nx.to_scipy_sparse_array(G.to_undirected()))\n",
    "    print(f\"N칰mero de componentes conexas: {n_components}\")\n",
    "    print(f\"Modularidad (Louvain): {community_louvain.modularity(partition, G.to_undirected(), weight='weight')}\")\n",
    "\n",
    "    return results_df, z\n",
    "\n",
    "# Ejecutar con todos los datos necesarios\n",
    "# Asumiendo que model, G, test_data, emprendimientos, tematicas, emprendimiento_tematica est치n definidos\n",
    "communities_df, embeddings = detect_communities_with_embeddings(\n",
    "    model, G, test_data, \n",
    "    emprendimientos, tematicas, emprendimiento_tematica,\n",
    "    n_clusters=5, resolution=1.0, use_3d=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
